{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What's a supercomputer??\n",
    "\n",
    "Let's start with a little entertainment.  Thanks Anastasi.\n",
    "\n",
    "https://www.youtube.com/watch?v=HvJGsF4t2Tc\n",
    "\n",
    "Let's break down some of these numbers:\n",
    "\n",
    "* Exascale -- $10^{18}$ FLOPS/s\n",
    "    * Terascale -- $10^{12}$ FLOPS/s in 1996\n",
    "    * Petascale -- $10^{15}$ FLOPS/s in 2008   \n",
    "* 52.5 GFlops/W -- just a big number.  but best ever.\n",
    "    * there is no way to make the biggest computer without making the most energy efficient computer \n",
    "* 6nm -- first production chip that I've heard of at 6 nm.  Appparently, 3nm will start in 2023.\n",
    "* 400 $m^2$ cabinets, 40mW  that seems small\n",
    "    * Inner-Mongolia data park is $1Mm^2$ and 150mW\n",
    "    * Switch Citadel is $1.3Mm^2$ and 130mW\n",
    "    * fun to watch https://www.youtube.com/watch?v=g7JaN3rTK2A\n",
    "* Water temp of 30 degree and 30,000 L/min\n",
    "    * this is a very small stream ~18 cfs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flynn's Taxonomy \n",
    "\n",
    "Characterize machines by number of instruction streams and data streams\n",
    "  * Defined in 1972.  Still common practice.\n",
    "  * A little too restrictive, but a starting place\n",
    "\n",
    "### SISD: single instruction, single data\n",
    "\n",
    "<img src=\"./images/sisd.png\" width=\"300\" title=\"SISD http://arstechnica.com/paedia/c/cpu/part-1/cpu1-1.html\" /><img src=\"./images/vn.png\" width=\"300\" title=\"Unknown source.\" />\n",
    "\n",
    "Perform  a stream on instructions on a stream of data\n",
    "  * The von Neumann architecture\n",
    "  * Conforms to serial algorithmic analysis\n",
    "\n",
    "### SIMD: single instruction, multiple data\n",
    "\n",
    "<img src=\"./images/simd.png\" width=\"512\" title=\"SISD http://arstechnica.com/paedia/c/cpu/part-1/cpu1-1.html\" />\n",
    "\n",
    "* Single control stream\n",
    "  * All processors execute the same instruction at the same time\n",
    "  * against different data\n",
    "  * synchronous execution or 'in lock step'\n",
    "* Fine-grained parallelism without inter-process communication\n",
    "* Examples  \n",
    "  * Intel vector processors\n",
    "  * GPU stream processor\n",
    "\n",
    "\n",
    "### MISD: multiple instruction, single data\n",
    "\n",
    "As a taxon, this class is irrelevant.  No such machines.\n",
    "  * Machines called systolic arrays used some principles\n",
    "  * The Google TPU has some of these principles.\n",
    "The concept may get used in more complex systems.\n",
    "\n",
    "### MIMD: multiple instruction, multiple data\n",
    "\n",
    "<img src=\"./images/mimd.png\" width=\"512\" title=\"Unknown source.\" />\n",
    "\n",
    "* Asycnchronous parallelism\n",
    "  * Mutliple processing units\n",
    "  * Each processing unit gets independent input/data streams\n",
    "* Most of the machines that we are interested in are MIMD + complex processing units\n",
    "  * GPU is a MIMD of SIMD processors\n",
    "  * NUMA is a MIMD of MIMD proessors\n",
    "  * HPC is a MIMD of MIMD of MIMD and SIMD processors\n",
    "  \n",
    "Flynnâ€™s taxonomy not so useful.  We must further divide the world by architectural features and programming model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Distributed Memory Systems\n",
    "\n",
    "<img src=\"https://hpc.llnl.gov/sites/default/files/distributed_mem.gif\" width=\"512\" title=\"Non-Uniform Memory Architecture\" />\n",
    "\n",
    "MIMD machines in which the processing units have their own private memory.  The only way to share data among nodes is to moved it explicitly from one memory to another.\n",
    "* _Message passing_\n",
    "  * A programming pattern in which parallel nodes intermix computation with sending and receiving data from other nodes.\n",
    "  * MPI (message-passing interface) is the standard programming environment to implement this pattern.\n",
    "* Cloud frameworks create programming environments that map computations to distributed memory\n",
    "  * Hadoop!(Map/Reduce): data-parallel programs over file I/O\n",
    "  * Spark: data-parallel programs over memory-resident partitioned data structures\n",
    "  * Dask\n",
    "  * Ray\n",
    "  \n",
    "### Hybrid Architectures\n",
    "\n",
    "\n",
    "<img src=\"https://hpc.llnl.gov/sites/default/files/hybrid_model.gif\" width=\"512\" title=\"Non-Uniform Memory Architecture\" />\n",
    "\n",
    "Hierarchical combination of multiple architectures.\n",
    "* Simple cloud or simple cluster \n",
    "  * distributed memory system consisting of\n",
    "  * SMP or NUMA nodes.\n",
    "* HPC or machine learning cloud\n",
    "  * distributed memory system consisting of\n",
    "  * SMP or NUMA nodes\n",
    "  * that have GPU, FPGA, Sunway, or Fujitsu/ARM accelerators\n",
    "  \n",
    "<img src=\"https://hpc.llnl.gov/sites/default/files/hybrid_mem2.gif\" width=\"512\" title=\"Non-Uniform Memory Architecture with GPU\" />\n",
    "\n",
    "The key challenge in modern parallel programming is efficiently mapping problems into codes that run efficiently on hybrid architectures.\n",
    "* The HPC approach is to write an MPI program that runs OpenMP programs on each node and makes calls to accelerators\n",
    "  * This is expensive and hard to maintain\n",
    "  * But a general solution and required to utilize hardware well\n",
    "* Machine learning frameworks have recently made great strides toward transparent support for hybrid systems\n",
    "  * PyTorch: NUMA+GPU\n",
    "  * MxNet: cloud+NUMA+GPU\n",
    "  * Keras\n",
    "  * Tensor Flow\n",
    "  * Others (I'm not following closely)\n",
    "  * a great path forward for compute-dense iterative models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
