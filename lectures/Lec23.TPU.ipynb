{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fceded0d-cbec-41bd-a2a6-edb69ee0d7e1",
   "metadata": {},
   "source": [
    "## Lecture 24: The Google TPU and the Evolution of Accelerators\n",
    "\n",
    "Lecture derived mostly from [Jouppi et al. _In-Datacenter Performance Analysis of a Tensor Processing Unit_. ISCA, 2017.](https://arxiv.org/pdf/1704.04760.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2159ab-77f7-430f-a91f-cb37963fcbb2",
   "metadata": {},
   "source": [
    "### Unpacking the abstract\n",
    "\n",
    "> Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. \n",
    "\n",
    "This is not at all an obvious assertion. Let's come back to it. **\n",
    "\n",
    "> custom ASIC—called a Tensor Processing Unit (TPU)—deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). \n",
    "\n",
    "ASIC == application-specific integrated circuit. \n",
    "\n",
    "This is a somewhat general term that captures a lot of different chips. The hardware is specfically designed for an individual task, rather than a general purpose computing platform for many programs. In this case, it means that Google built a new chip that is fully-customized, i.e. defined from the photolithography up. \n",
    "\n",
    "_Inference phase_: This is important. Neural networks operate in two modes:\n",
    "  * training: run examples through the neural networks, back propagate to determine error, gradient descent to udpate weights. (and perhaps a bunch of other stuff)\n",
    "  * inference: evaluate an input on a trained neural network to make a classification or regression decision\n",
    "  \n",
    "Training has always been the limiting factor in NN scalability. Researchers focus on training performance, in pursuit of the highest accuracy NN.\n",
    "\n",
    "Google has a different problem. They service many inference requests on already trained models. It is a simpler computation, but it is being done at massive scale.\n",
    "\n",
    "> The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS).\n",
    "\n",
    "* TOPS -- they are not FLOPS because they are 8-bit\n",
    "* 92T compares with <3T for GPU or CPU at the time\n",
    "* it is a 256x256 array for 2-d MM\n",
    "\n",
    "> The TPU’s deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs (caches, out-of-order execution, multithreading, multiprocessing, prefetching, ...)\n",
    "\n",
    "If you have a predictable and simple program:\n",
    "  * no branching\n",
    "  * no stack (no function calls)\n",
    "a simpler allocation of resources can lead to better price/power/performance.\n",
    "\n",
    "GPU simplifies over CPU.\n",
    "  * eliminates stack\n",
    "  * eliminates most managed cache\n",
    "  * no branch prediction or OOE\n",
    "  \n",
    "TPU simplifies over GPU\n",
    "  * eliminates all cache\n",
    "  * supports only 8-bit arithmetic ops\n",
    "  * no threads, warps\n",
    "  * no pipelining or interleaved execution\n",
    "  \n",
    "Essentially, the TPU is a coprocessor that does a single structured computation one at a time.\n",
    "\n",
    "> that help average throughput more than guaranteed latency.\n",
    "\n",
    "This connects to the latency goal. The performance target is the time to perform a single inference. The architecture is well suited to this task. Deterministic computation is predictable and will always meet latency targets. \n",
    "\n",
    ">The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. .........'\n",
    "\n",
    "Not surprised. This is a remarkably limited tool. Lean and mean.\n",
    "\n",
    "> NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters’ NN inference demand. \n",
    "\n",
    "This is the real secret. Google has a few classes of ML models and a very specific workload. They want to run inference on neural networks interactively, i.e. a single inference at a time, with minimum latency.  \n",
    "\n",
    "The trend of customizing HW to SW workloads is sometimes called **co-design** and is a hot topic in AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96418c4b-76dc-46c8-ac8b-b74f719e42ef",
   "metadata": {},
   "source": [
    "### The Target (OK Google)\n",
    "\n",
    ">  people use voice search for 3 minutes a day using speech recognition DNNs would require our datacenters to double to meet computation demands\n",
    "\n",
    "The evolution of Google's services toward voice interaction transformed their computing needs.  They used to run ML in the corners or their big clusters. With voice, ML became the \\#1 user of compute resources. It became worthwhile to build an entirely new chip for this specific application\n",
    "\n",
    "\n",
    "### The Secret (reduced precision)\n",
    "\n",
    "> A step called quantization transforms floating-point numbers into narrow integers—often just 8 bits—which are usually good enough for inference.\n",
    "\n",
    "> Eight-bit integer multiplies can be 6X less energy and 6X less area than IEEE 754 16-bit floating-point multiplies, and the advantage for integer addition is 13X in energy and 38X in area\n",
    "\n",
    "These observations lead to a design point.\n",
    "  * train models on full-precision hardware (GPUs)\n",
    "  * quantize models\n",
    "  * deploy models on low-precision hardware (TPUs)\n",
    "  \n",
    "This is exactly what Google was doing in 2017. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50733a68-40ec-42ad-a669-7dbc9f7e9b27",
   "metadata": {},
   "source": [
    "### The TPU as an Accelerator\n",
    "\n",
    ">TPU was designed to be a coprocessor on the PCIe I/O bus, allowing it to plug into existing servers just as a GPU does. \n",
    "\n",
    "<img src=\"./images/tpublockdiagram.png\" width=512 />\n",
    "\n",
    "> Moreover, to simplify hardware design and debugging, the host server sends TPU instructions for it to execute rather than fetching them itself.\n",
    "\n",
    "This is a very different design point from the GPU.\n",
    "  * GPU -- transfer a program (kernel) that is executed remotely\n",
    "  * TPU -- execute instructions on the TPU\n",
    "    * instruction buffer queues instructions to avoid latency\n",
    "    * but, CPU must initiate each instruction\n",
    "    \n",
    "Instruction throughput is a function of input precision:\n",
    "  * full speed: 8-bit weights and 8-bit activations\n",
    "  * half speed: 8 and 16\n",
    "  * quarter speed: 16 and 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b62527-3726-49b2-a94f-bd2ed7ea14bf",
   "metadata": {},
   "source": [
    "### Silicon real estate\n",
    "\n",
    "Layout reflects simplicity of computations. No managed caches. Little control hardware.\n",
    "  * 37% of area for memory (28+ MB)\n",
    "  * 30% of area for computation\n",
    "  \n",
    "<img src=\"./images/tpuareal.png\" width=512 />\n",
    "  \n",
    "This memory is on-chip. It should not be compared with GPU memory (DRAM). Rather, it is more analagous to the combination of GPU registers, L1, and L2.\n",
    "\n",
    "Memory is used to progammatically reuse data:\n",
    "  * activations: outputs of one layer are inputs to next layer\n",
    "  \n",
    "Weights prefetched and streamed onto MAC:\n",
    "  * TPU has an off-chip 8 GiB DRAM for weights\n",
    "  * Weights are static: modeled is already trained\n",
    "  * Needed data is known ahead of time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f644221-3787-4d38-b3b7-3087bdb33206",
   "metadata": {},
   "source": [
    "### TPUs follow the CISC tradition!!!!\n",
    "\n",
    "This is a huge departure from modern practice. The goal of modern CPU design has been to maximize instruction level parallelism (ILP). The RISC (Reduced Instruction Set Computer) architecture pushed this agenda, starting in the 1980s. The paper summarizes it in one line\n",
    "\n",
    "> traditional RISC pipeline with one clock cycle per stage\n",
    "\n",
    "CISC (complex instruction set computer) has negative connotations because variable and unpredictable instruction execution times reduce ILP. The CISC/RISC debate is 40 years old and not that informative. As we learned in the midterm, X86 instructions are not \"reduced\" and not \"predictable\".\n",
    "\n",
    "However, X86 wants to maximize instruction level parallelism through RISC-like principles:\n",
    "  * pipelines\n",
    "  * out-of-order execution\n",
    "\n",
    "And, other approaches.\n",
    "  * vector processing\n",
    "  * fused multiple/add\n",
    "  \n",
    "This is not what the TPU is doing (for the most part). The TPU has 5 main instructions each that takes 10-20 clock cycles. The TPU does benefit from pipelining and overlapping I/O with computation in the MAC. It does reads and writes asychronoulsy and it runs the activation and pool steps in parallel. The whole processes is limited by the throughput of matrix multiply. \n",
    "\n",
    "Matrix multiply is done using a \"systolic\" array:\n",
    "\n",
    "> A given 256-element multiply-accumulate operation moves through the matrix as a diagonal wavefront. The weights are preloaded, and take effect with the advancing wave alongside the first data of a new block. Control and data are pipelined to give the illusion that the 256 inputs are read at once, and that they instantly update one location of each of 256 accumulators.\n",
    "\n",
    "<img src=\"./images/tpusystolic.png\" width=512 />\n",
    "\n",
    "Systolic is an arcane term that goes back to WWII codebreaking. It essentially means that you are passing data over multiple processing units. It has been questionably connected to the MISD (multiple instruction single data) taxon of Flynn's taxonomy.\n",
    "\n",
    "The taxonomy confuses the issues.  The TPU initiates and completes a 256x256 MM every clock cycle. Think of it as a 2-d pipeline. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce09ea3-f242-4d89-a258-4fcf62dde7f4",
   "metadata": {},
   "source": [
    "### Roofline Performance\n",
    "\n",
    "We've already looked at this. The TPU:\n",
    "  * has a roofline corner that requires high operational intensity\n",
    "  * results in efficient kernels that approach the roofline\n",
    "  * exceeds CPU and GPU performance on neural networks\n",
    "\n",
    "<img src=\"./images/tpuroofline.png\" width=512 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389b8e4-97df-43ad-a9c9-568b13d7c079",
   "metadata": {},
   "source": [
    "### What's happened since?\n",
    "\n",
    "* 2017 TPU generation 2\n",
    "  * High-bandwidth memory (600 GB/s). Push the Roofline corner left.\n",
    "  * Floating point. Suitable for training as well as inference.\n",
    "  * Deployed in pods: 4 TPUs x 64 modules\n",
    "  \n",
    "* 2018 TPU generation 3\n",
    "  * 2x TOPS, 4x pods = 8x performance per pod\n",
    "  \n",
    "* 2018 Edge TPU\n",
    "  * 8-bit, lower power, inference.\n",
    "  \n",
    "* 2019 Pixel 4 with Edge TPU (Pixel Neural Core)\n",
    "  \n",
    "* 2021 TPU v4.\n",
    "    * pods of 4096 TPUs\n",
    "    * 10x bandwidth\n",
    "    * exaflops of floating point 16 operations per pod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372c2213-15e3-472d-945e-12e78e745568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
