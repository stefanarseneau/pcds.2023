{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711b9f4c-ee1d-4c5f-9cb8-e8c0e6cf7b66",
   "metadata": {},
   "source": [
    "## Ray: Task Programming with Remote Functions\n",
    "\n",
    "> to install Ray run\n",
    "```\n",
    "  conda install pip\n",
    "  pip install ray\n",
    "```\n",
    "\n",
    "> to install on M1 MacOSX\n",
    "```\n",
    "  conda install -c conda-forge grpcio\n",
    "  conda install pip\n",
    "  pip install ray\n",
    "```\n",
    "\n",
    "Latest build.\n",
    "```\n",
    "  pip install grpcio pygrpc\n",
    "  pip install -U \"ray[default]\"\n",
    "```\n",
    "\n",
    "So far we have looked at cloud programming tools geared towards data parallel applications driven by a data decomposition. But, as well recall from Chapter 2 of Mattson, there is also a task decomposition. Data decomposition matches the data science, groupby analysis workloads of Internet/cloud data processing, such as processing logs, crawling and indexing the Web. However, \"modern AI\" applications have different demands.\n",
    "\n",
    "The paper [Ray: A Distributed Framework for Emerging AI Applications](https://arxiv.org/pdf/1712.05889.pdf) takes a different approach. The abstract is confusing:\n",
    "\n",
    "> The next generation of AI applications will continuously\n",
    "interact with the environment and learn from these interactions. These applications impose new and demanding\n",
    "systems requirements, both in terms of performance and\n",
    "flexibility. In this paper, we consider these requirements\n",
    "and present Ray—a distributed system to address them.\n",
    "Ray implements a unified interface that can express both\n",
    "task-parallel and actor-based computations, supported by\n",
    "a single dynamic execution engine. To meet the performance requirements, Ray employs a distributed scheduler\n",
    "and a distributed and fault-tolerant store to manage the\n",
    "system’s control state.\n",
    "\n",
    "All one really gets out of this is that it is a distributed task/actor programming model. IMO, this is an example of poor, underspecific writing. My statement on Ray is that:\n",
    "\n",
    "  > RB: Data parallel frameworks, such as Dask and Spark, do not capture the task/actor decomposition needed to encode parallelism for modern AI applications.\n",
    "  \n",
    "OK, that's not perfect either. As we dig into the paper, we find more specific statements that shed light on the issues:\n",
    "\n",
    "> They must aim not only to exploit the data gathered, but also to explore the space of possible actions. \n",
    "\n",
    "> These characteristics drive new systems requirements:\n",
    "a system for RL must support fine-grained computations\n",
    "(e.g., rendering actions in milliseconds when interacting\n",
    "with the real world, and performing vast numbers of simulations), must support heterogeneity both in time (e.g.,\n",
    "a simulation may take milliseconds or hours) and in resource usage (e.g., GPUs for training and CPUs for simulations), and must support dynamic execution, as results of simulations or interactions with the environment can change future computations.\n",
    "\n",
    "The paper uses reinforcement learning as the singular motivation. It's a good and important example. However, the generality of the framework does not match the specificity of the motivating application. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf33f6-8d07-4d38-9555-0861ddaaa115",
   "metadata": {},
   "source": [
    "### Remote Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac86900-2313-4137-8f9b-f85186373e1c",
   "metadata": {},
   "source": [
    "These are simple examples take from the [Ray tutorial](https://github.com/ray-project/tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1488ee1b-0003-4be7-a3dd-3b18c6a2d015",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported ray!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import ray\n",
    "import time\n",
    "\n",
    "print('Successfully imported ray!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2678aaf1-6e5c-434f-bf69-ea1afac8012a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 16:53:41,446\tWARNING services.py:1826 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67108864 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=2.29gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2023-06-10 16:53:41,568\tINFO worker.py:1636 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "ray.init(num_cpus=4, ignore_reinit_error=True)\n",
    "\n",
    "# Sleep a little to improve the accuracy of the timing measurements used below,\n",
    "# because some workers may still be starting up in the background.\n",
    "time.sleep(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9591208-9059-436e-adfa-06aa5960395c",
   "metadata": {},
   "source": [
    "Run a trivial program in serial to show that it is slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc077975-7d7c-4d29-858e-d5584e1b8a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the for loop took 4.007 seconds.\n",
      "The results are: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# A function simulating a more interesting computation that takes one second.\n",
    "def slow_function(i):\n",
    "    time.sleep(1)\n",
    "    return i\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "results = []\n",
    "for i in range(4):\n",
    "  results.append(slow_function(i))\n",
    "  \n",
    "duration = time.time() - start_time\n",
    "print('Executing the for loop took {:.3f} seconds.'.format(duration))\n",
    "print('The results are:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666e28f9-eabc-4fe3-95e4-ba386ce9dc83",
   "metadata": {},
   "source": [
    "Parallelize the same computation with remote functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a57fd7a-642b-46fe-9151-88a50801f95b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing the for loop took 1.018 seconds.\n",
      "The results are: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "@ray.remote\n",
    "def slow_function(i):\n",
    "    time.sleep(1)\n",
    "    return i\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "results = []\n",
    "oids = []\n",
    "\n",
    "for i in range(4):\n",
    "   oids.append(slow_function.remote(i))\n",
    "\n",
    "for i in range(4):\n",
    "   results.append(ray.get(oids[i]))\n",
    "  \n",
    "duration = time.time() - start_time\n",
    "print('Executing the for loop took {:.3f} seconds.'.format(duration))\n",
    "print('The results are:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f4d7d-8377-4af7-b653-80d52799d5bf",
   "metadata": {},
   "source": [
    "Good. A trivially parallelizeable program has a speedup of 4-$\\epsilon$ on four cores. Ray has some similarities to joblib. It creates functions that run asynchronously. The Ray specific items here are:\n",
    "  * `@ray.remote` decorator that indicates that this is a remote function.\n",
    "  * `slow_functions.remote(i)` invoke the function to run asynchronously \n",
    "  * `ray.get(oids[i])` get the return value of the remote function\n",
    "\n",
    "Ray is a runtime system that is managing a cluster, just like Dask and Spark. Calling `.remote()` on a function sends it to the cluster for execution. Ray will package the entire function and all the data needed and send it to the executor.\n",
    "\n",
    "You will also notice that this programming pattern is similar to the fork/join pattern used in Java threads, i.e. launch asynchronous executors and then join them serially. `ray.get()` awaits the completion of a remote function. The loop of `ray.get()` joins remote functions serially. It completes once all of the remote functions complete.\n",
    "\n",
    "Ray functions are **stateless**. They take arguments and return something. Their operations has no side effects and there is no shared state.\n",
    "\n",
    "### Remote function representation\n",
    "\n",
    "`ray.remote()` returns an object identifier that is a handle to the function. The reference can be used to query and manipulate the remote function. We will see that references create the capability to build complex dependency chains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb125466-67d1-471b-95d9-09e408a8b067",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ObjectRef(c8ef45ccd0112571ffffffffffffffffffffffff0100000001000000), ObjectRef(16310a0f0a45af5cffffffffffffffffffffffff0100000001000000), ObjectRef(c2668a65bda616c1ffffffffffffffffffffffff0100000001000000), ObjectRef(32d950ec0ccf9d2affffffffffffffffffffffff0100000001000000)]\n"
     ]
    }
   ],
   "source": [
    "print(oids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fabf939-14f1-4145-b853-7c9079c135ef",
   "metadata": {},
   "source": [
    "### Distributing Data to remote functions\n",
    "\n",
    "Let's revisit our mutual outlinks example from joblib in Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c675c9bc-cb71-4d81-9c06-988489c8bedd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10166"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "G= nx.erdos_renyi_graph(1000,0.01,directed=True)\n",
    "#nx.draw(G, pos=nx.spring_layout(G), node_size=10)\n",
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f4b5e1d-2b6e-435e-9e45-388c15e0d220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gmat = nx.to_numpy_matrix(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9a0c3-e7d2-49e1-b378-5bcaa28c690f",
   "metadata": {},
   "source": [
    "### Serial Reference Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1c028d1-0b6a-4e3b-906c-5ad064e4bdd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49043"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "gmat = nx.to_numpy_matrix(G)\n",
    "\n",
    "outmat = np.zeros(gmat.shape)\n",
    "\n",
    "for i in range(gmat.shape[0]):\n",
    "    for j in range(i+1,gmat.shape[1]):        \n",
    "        outmat[i,j] = np.dot(np.asarray(gmat[i,:]).reshape(-1), np.asarray(gmat[j,:]).reshape(-1))\n",
    "        \n",
    "np.count_nonzero(outmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed9ce36-ea39-4f8d-b2cc-ccb96ff7f74a",
   "metadata": {},
   "source": [
    "### Ray `remote()` implementation and timing\n",
    "\n",
    "We will do the dumbest thing, which will work. The remote function will refer to `gmat` as a global variable. Given that Ray is process based (not thread based) there is no shared memory and so Ray is doing something smart to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7532409e-8fa3-497e-811b-38dfef5186fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49043"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@ray.remote\n",
    "def inner_loop(i):\n",
    "    partial_out = np.zeros(gmat.shape[1])\n",
    "    for j in range(i+1,gmat.shape[1]):    \n",
    "        partial_out[j] = np.dot(np.asarray(gmat[i,:]).reshape(-1), np.asarray(gmat[j,:]).reshape(-1))    \n",
    "    return partial_out\n",
    "    \n",
    "oids = []\n",
    "results = []\n",
    "\n",
    "for i in range(gmat.shape[0]):\n",
    "    oids.append(inner_loop.remote(i))\n",
    "    \n",
    "for i in range(gmat.shape[0]):    \n",
    "   results.append(ray.get(oids[i]))\n",
    "\n",
    "outmat = np.array(results)\n",
    "np.count_nonzero(outmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75a096a5-87e7-499d-8b7c-09c8d7ce681e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.05 s ± 11.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "oids = []\n",
    "results = []\n",
    "\n",
    "for i in range(gmat.shape[0]):\n",
    "    oids.append(inner_loop.remote(i))\n",
    "    \n",
    "for i in range(gmat.shape[0]):    \n",
    "   results.append(ray.get(oids[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778c2e6e-f575-4b3b-8a03-86244b07289e",
   "metadata": {},
   "source": [
    "OK, let's compare the performance with joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3bd908fe-abef-44e1-b179-070f59b8692b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49043"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def inner_loopj(i):\n",
    "    partial_out = np.zeros(gmat.shape[1])\n",
    "    for j in range(i+1,gmat.shape[1]):    \n",
    "        partial_out[j] = np.dot(np.asarray(gmat[i,:]).reshape(-1), np.asarray(gmat[j,:]).reshape(-1))    \n",
    "    return partial_out\n",
    "    \n",
    "from joblib import Parallel, delayed\n",
    "partials = Parallel(n_jobs=4)(delayed(inner_loopj)(i) for i in range(gmat.shape[0]))\n",
    "\n",
    "outmat = np.array(partials)\n",
    "np.count_nonzero(outmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93e2f6ef-edcb-4cc4-a021-b2ad8ab90540",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.76 s ± 114 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "partials = Parallel(n_jobs=4)(delayed(inner_loopj)(i) for i in range(gmat.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b166c3-9d79-443a-845a-67d456e696f3",
   "metadata": {},
   "source": [
    "OK, so ray is a bunch faster.  Why?\n",
    "\n",
    "### Ray Memory Management\n",
    "\n",
    "Ray implements an object store for remote functions to share memory. Instead of copying data to each process, Ray puts into a an object store and let's the processes access them. This can either be Redis when functions are distributed across multiple nodes or in [Plasma](https://ray-project.github.io/2017/08/08/plasma-in-memory-object-store.html) which provides shared memory for processes on the same machine.\n",
    "\n",
    "This is consistent with stateless functions. Ray is packaging the needed data (global variables) and sending them to the functions. Not surprisingly, this data must be read only.\n",
    "\n",
    "We can make Ray really slow by making it copy memory, passing the whole array as a function argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3be9ac-adfe-499d-a975-f1b940366b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def inner_loop2(i, gmat):\n",
    "    partial_out = np.zeros(gmat.shape[1])\n",
    "    for j in range(i+1,gmat.shape[1]):    \n",
    "        partial_out[j] = np.dot(np.asarray(gmat[i,:]).reshape(-1), np.asarray(gmat[j,:]).reshape(-1))    \n",
    "    return partial_out\n",
    "    \n",
    "oids = []\n",
    "results = []\n",
    "\n",
    "for i in range(gmat.shape[0]):\n",
    "    oids.append(inner_loop2.remote(i, gmat))\n",
    "    \n",
    "for i in range(gmat.shape[0]):    \n",
    "   results.append(ray.get(oids[i]))\n",
    "\n",
    "outmat = np.array(results)\n",
    "np.count_nonzero(outmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9751bc89-4f21-4874-8fc0-637640f2aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "oids = []\n",
    "results = []\n",
    "\n",
    "for i in range(gmat.shape[0]):\n",
    "    oids.append(inner_loop2.remote(i,gmat))\n",
    "    \n",
    "for i in range(gmat.shape[0]):    \n",
    "   results.append(ray.get(oids[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68cc748-5936-4933-a953-7044efc0a45b",
   "metadata": {},
   "source": [
    "We can replicate Ray's performance of handling global variables by put the matrix into the object store. This is `ray.put()`.\n",
    "\n",
    "In remote functions, object references passed as arguments will fetched from the object store and available as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41b2185-b785-4bf0-82ec-1811aa4384ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def inner_loop3(i, gmat):\n",
    "    partial_out = np.zeros(gmat.shape[1])\n",
    "    for j in range(i+1,gmat.shape[1]):    \n",
    "        partial_out[j] = np.dot(np.asarray(gmat[i,:]).reshape(-1), np.asarray(gmat[j,:]).reshape(-1))    \n",
    "    return partial_out\n",
    "\n",
    "gmat_ref = ray.put(gmat)\n",
    "\n",
    "oids = []\n",
    "results = []\n",
    "\n",
    "for i in range(gmat.shape[0]):\n",
    "    oids.append(inner_loop3.remote(i, gmat_ref))\n",
    "    \n",
    "for i in range(gmat.shape[0]):    \n",
    "   results.append(ray.get(oids[i]))\n",
    "                  \n",
    "outmat = np.array(results)\n",
    "np.count_nonzero(outmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3819afde-8041-4d1d-b89f-7f4a028039a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "oids = []\n",
    "results = []\n",
    "\n",
    "for i in range(gmat.shape[0]):\n",
    "    oids.append(inner_loop3.remote(i,gmat_ref))\n",
    "    \n",
    "for i in range(gmat.shape[0]):    \n",
    "   results.append(ray.get(oids[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71955d7a-37d1-4b10-9026-b2b865c946df",
   "metadata": {},
   "source": [
    "If we compare this implementation to the slow implementation.  This is a _pass by reference_ and the slow one is _pass by value_.\n",
    "\n",
    "Overall, it's pretty cool that Ray will take global variables that are using in functions and transparently marshall them into an object store.\n",
    "\n",
    "* Ray serializes/deserializes objects, giving the appearance of shared memory.\n",
    "    * the shared memory abstraction for global variables is both good and bad.\n",
    "    * _good_: makes programming easy. prevents \n",
    "    * _bad_ : implicit data movement makes performance opaque\n",
    "\n",
    "## Outcomes\n",
    "\n",
    "  * Ray is built on distributed objects\n",
    "  * Ray has a `put()` and `get()` interface for objects that can be functions or data\n",
    "      * `get()` on a remote function waits for completion and fetches its return values\n",
    "  * The `ray.remote` decorator makes a function suitable for parallel execution\n",
    "  * The `remote()` call starts asynchronous execution of a function.\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
